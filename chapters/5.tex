\chapter{Signal flow diagrams} \label{ch.sigflow}
In this chapter we use corelations to guide the development of a graphical
language for reasoning about linear time-invariant systems. 

%The work builds on recent of work Bonchi, Soboci\'nski, and Zanasi, in which
%they develop a sound and complete graphical language for reasoning about linear
%relations \cite{BSZ1}. In their work they provide a presentation of the category
%of linear relations over the field of Laurent polynomials $k(s)$ in some formal
%variable $s$ over some field $k$, and operational semantics In the previous part, we observed that this
%category of linear relations can be constructed as epi-mono corelations over the
%category $\Mat k(s)$ of matrices with entries in $k(s)$.  
%
%In the work of Willems, however, it is matrices over the ring of polynomials $k[s,s^{-1}]$ in $s$ and its formal inverse
%$s^{-1}$ that plays the central role in representing 
%

We begin in the next section with motivation and an overview of this
chapter. In \S\ref{sec.systems} we then develop a categorical account of
complete LTI discrete dynamical systems. This serves as a denotational semantics
for the graphical language, introduced in \S\ref{sec.diagrams}, where we also
derive the equational characterisation.  In \textsection\ref{sec.opsem} we
relate this to an operational semantics, in terms of biinfinite streams of
elements of $k$. We conclude in \S\ref{sec.control} with a structural account of
controllability.


\section{Behavioural control theory}

Control theory begins with the following picture:		
\[
\begin{tikzpicture}
\node(system) [shape=rectangle,draw,inner sep=20pt] at (0,0) {\textsf{system}};
\node(input) at (-3.5,0) {\small{\textsf{input}}};
\node(output) at (3.5,0) {\small{\textsf{output}}};
\draw [->,shorten >=5pt,>=stealth,very thick] (input) 	-- (system);
\draw [->,shorten <=5pt,>=stealth,very thick] (system)	-- (output);
\end{tikzpicture}
\]
In this picture we have an object under study, referred to as a \emph{system}, which when fed certain inputs produces certain outputs. These outputs need not be uniquely determined by the inputs; in general the relationship may be stochastic or non-deterministic, or depend also on internal states of the system. Although we take it as given that the system is an \emph{open} system---so it interacts with its environment, and so we can observe its inputs and outputs---we assume no access to the details of these internal states or the inner workings of the system, and this can add considerable complexity to our models. The end goal of control theory is then to control the system: to understand how to influence its behaviour in order to achieve some desired goal. Two key questions arise: 
\begin{itemize}
\item \textbf{Analysis}: Given a system, what is the relationship it induces between input and output?
\item \textbf{Synthesis}: Given a target relationship between input and output, how do we build a system that produces this relationship?
\end{itemize}
The first, the question of system analysis, provides the basic understanding
required to find the inputs that lead to the desired outputs. The second
question, the question of synthesis, is the central question of feedback control
theory, which aims to design controllers to regulate other systems. We give a
brief overview of the field, illustrated with some questions of these kinds.

Control has a long history. Indeed, many systems found in the biology and chemistry of living organisms have interesting interpretations from a control theoretic viewpoint, such as those that are responsible for body temperature regulation or bipedal balance \cite{So2}. Human understanding of control developed alongside engineering, and so dates back at least as far as antiquity, with for example the ancient Romans devising elaborate systems to maintain desired water levels in aqueducts \cite{So}. The origin of formal mathematical control theory, however, is more recent, and in general taken to be James Clerk Maxwell's seminal analysis of centrifugal governors, presented to the Royal Society of London in 1868 \cite{CM}. 

The techniques used and developed from Maxwell's paper, in particular by Rayleigh and Heaviside, are in general known as \emph{transfer function} techniques. A transfer function is a linear map from the set of inputs to the set of outputs of a linear time-invariant system with zero initial conditions. Transfer functions are commonly used in the analysis of single-input single-output linear systems, but become unwieldy or inapplicable for more general systems.

In the 1960s, led by Wiener and Kalman, so-called \emph{state space} techniques were developed to address multiple-input multiple-output, time-varying, nonlinear systems \cite{Fr}. These methods are characterised by defining a system as a collection of input, output, and internal state variables, with the state changing as a function of the input and state variables over time, and the output a function of the input and state. These functions are often implicitly specified by differential equations.

In general, however, classical control theory remains grounded in a paradigm that defines and analyses systems in terms of inputs and outputs or, from another perspective, causes and effects. In recent years Willems, among others, has argued that this input-output perspective is limiting, as in the case of many systems studied through control theory there is no clear distinction between input and output variables \cite{Wi}. For example, given a circuit component for which the relevant variables are the voltages and currents, different contexts may call for the voltage to be viewed as the input and current output, or vice versa. It is useful to have a single framework capable of discussing the behaviour of the component without making this choice.

Moreover, in the drive to understand larger and more complex systems, increasing
emphasis has been put on understanding the way systems can be broken down into
composite subsystems, and conversely how systems link together to form larger
systems \cite{Wi2, KT}. Indeed, interconnection or composition of systems has
always played a central role in systems engineering, and the
difficulty of discussing how systems compose within an input-output framework
lends support to Willems' call for a more nuanced definition of control system.
To illustrate these difficulties, consider the simple example, due to Willems,
of two water tanks each with two access pipes:
\[
\begin{tikzpicture}
    % draw the tanks
    \begin{pgfonlayer}{main}
    \draw (-4,.1)--(-3,.1)--(-3,0)--(-1,0)--(-1,.1)--(0,.1);
    \draw (-4,.3)--(-3,.3)--(-3,2)--(-1,2)--(-1,.3)--(0,.3);
    \node at (-3,.3) [anchor=-55]{\scriptsize$\begin{array}{c} p_{A1} \\ f_{A1} \end{array}$};
    \node at (-1,.3) [anchor=-125]{\scriptsize$\begin{array}{c} p_{A2} \\ f_{A2} \end{array}$};
    \node at (-2,1){Tank $A$};
    \end{pgfonlayer}
    % fill them with water (in the background)
    \begin{pgfonlayer}{background}
        \filldraw[blue!20] (-4,.1)--(-3,.1)--(-3,0)--(-1,0)--(-1,.1)
        --(0,.1)--(0,.3)--(-1,.3)--(-1,1.8)--(-3,1.8)--(-3,.3)--(-4,.3)
        --cycle;
    \end{pgfonlayer}
\end{tikzpicture}
\qquad
\begin{tikzpicture}
    % draw the tanks
  \begin{pgfonlayer}{main}
    \draw (-4,.1)--(-3,.1)--(-3,0)--(-1,0)--(-1,.1)--(0,.1);
    \draw (-4,.3)--(-3,.3)--(-3,1.7)--(-1,1.7)--(-1,.3)--(0,.3);
    \node at (-3,.3) [anchor=-55]{\scriptsize$\begin{array}{c} p_{B1} \\ f_{B1} \end{array}$};
    \node at (-1,.3) [anchor=-125]{\scriptsize$\begin{array}{c} p_{B2} \\ f_{B2} \end{array}$};
    \node at (-2,1){Tank $B$};
    \end{pgfonlayer}
    % fill them with water (in the background)
    \begin{pgfonlayer}{background}
        \filldraw[blue!20] (-4,.1)--(-3,.1)--(-3,0)--(-1,0)--(-1,.1)
        --(0,.1)--(0,.3)--(-1,.3)--(-1,1.5)--(-3,1.5)--(-3,.3)--(-4,.3)
        --cycle;
    \end{pgfonlayer}
\end{tikzpicture}
\]
For each tank, the relevant variables are the pressure $p$ and the flow $f$. A typical control theoretic analysis might view the water pressure as inducing flow through the tank, and hence take pressure as the input variable, flow as the output variable, and describe each tank as a transfer function $H_\bullet: P_\bullet \to F_\bullet$ between the sets $P_\bullet$ and $F_\bullet$ these vary over. 

Ideally then, the composite system below, connecting pipe 2 of Tank $A$ to pipe 1 of Tank $B$, would be described by the composite of the transfer functions $H_A$ and $H_B$ of these tanks.
\[
\begin{tikzpicture}
    % draw the tanks
    \draw (-4,.1)--(-3,.1)--(-3,0)--(-1,0)--(-1,.1)--(1,.1)--(1,0)--(3,0)--(3,.1)--(4,.1);
    \draw (-4,.3)--(-3,.3)--(-3,2)--(-1,2)--(-1,.3)--(1,.3)--(1,1.7)--(3,1.7)--(3,.3)--(4,.3);
    % fill them with water (in the background)
    \begin{pgfonlayer}{background}
        \filldraw[blue!20] (-4,.1)--(-3,.1)--(-3,0)--(-1,0)--(-1,.1)
        --(1,.1)--(1,0)--(3,0)--(3,.1)--(4,.1)
        --(4,.3)--(3,.3)--(3,1.63)--(1,1.63)--(1,.3)
        --(-1,.3)--(-1,1.7)--(-3,1.7)--(-3,.3)--(-4,.3)
        --cycle;
    \end{pgfonlayer}
    \node at (-3,.3) [anchor=-55]{\scriptsize$\begin{array}{c} p_{A1} \\ f_{A1} \end{array}$};
    \node at (3,.3) [anchor=-125]{\scriptsize$\begin{array}{c} p_{B2} \\ f_{B2} \end{array}$};
    \node at (-1,.3) [anchor=-125]{\scriptsize$\begin{array}{c} p_{A2} \\ f_{A2} \end{array}$};
    \node at (1,.3) [anchor=-55]{\scriptsize$\begin{array}{c} p_{B1} \\ f_{B1} \end{array}$};
    \node at (-2,1){Tank $A$};
    \node at (2,1){Tank $B$};
\end{tikzpicture}
\]
This is rarely the case, and indeed makes little sense: the output of transfer
function $H_A$ describes the flow through Tank $A$, while the domain of the
transfer function $H_B$ describes pressures through Tank $B$, so taking the
output of $H_A$ as the input of $H_B$ runs into type issues. Instead, here
the relationship between the transfer functions $H_A$ and $H_B$, and the
transfer function $H_{AB}$ of the composite system can be understood through the
fact that connection requires that the pressure at pipe 1 of Tank $A$ must be equal to the pressure at pipe 2 of Tank $B$, and that the flow out of the former pipe must equal the flow into the latter---that is, by imposing the relations 
\begin{align*}
p_{A2} &= p_{B1} \\ f_{A2} &= - f_{B1}
\end{align*}
on the variables. Indeed in many contexts, hydraulics and electronics among them, connections between systems are characterised not by the `output' of one system forming the `input' of the next, but by \emph{variable sharing} between the systems. Such relations are often difficult to describe using the language of transfer functions and state space methods.

The themes of this thesis fit tightly into this programme of modelling control
theoretic systems with an emphasis on interconnection---Willems' so-called
behavioural approach. Willems first demonstrated this approach in the setting of
linear time-invariant discrete-time (LTI) dynamical systems \cite{Wi3}. A limitation
of the approach has been lack of formal language for representing and reasoning
about the interconnection of systems. In this chapter we address this by
developing a graphical language for LTI systems.

The expressions of this graphical language, closely resembling the signal flow
graphs of Shannon~\cite{Sh}, will form the morphisms of a category of
corelations, and its derivation as such will be crucial in developing a sound
and complete equational theory of LTI systems.  

To acquaint ourselves with signal flow graphs, we begin with the example below,
rendered in traditional, directed notation.
\begin{equation}\label{eq:examplesfg}
  \begin{aligned}
\lower11pt\hbox{$\includegraphics[height=2cm]{pics/examplesfg.pdf}$}
\end{aligned}
\end{equation}
This system takes, as input on the left, a stream of values from a field $\k$,
e.g.\ the rational numbers, and on the right outputs a processed stream of
values. The white circles are adders, the black circles are duplicators, the
$s$ gates are 1-step delays and the $-1$ gate is an instance of an amplifier
that outputs $-1$ times its input. Processing is done synchronously according to
a global clock. 

For instance, assume that at time $0$ the left $s$ gate `stores' the value $1$
and the right $s$ gate stores $2$. Given an input of $-1$, the flow graph first
adds the left stored value $1$, and then adds $-1 \times 2$, for an output of
$-2$. Immediately after this time step the $s$ gates, acting as delays, now
store $-1$ and $-2$ respectively, and we repeat the process with the next input.
Thus from this time $0$ an input stream of $-1,1,-1,1\dots$ results in an output
stream of $-2,2,-2,2,\dots$.

We can express \eqref{eq:examplesfg} as a string diagram by forgetting the
directionality of wires and composing the following basic building blocks using
the operations of monoidal categories:
\[
\copygen \mathrel{;} 
\!\!\!\!\!\!\!
\begin{array}{c} \delaygen \\ \oplus \\ \id \end{array} 
\!\!\!\!\!\!\!\mathrel{;}\!\! \addgen
\mathrel{;} \!\!\!\!\!\!\! \begin{array}{c} \discardopgen \\ \oplus \\ \id \end{array} 
\!\!\!\!\!
\mathrel{;} 
\!\!\!\!\!
 \begin{array}{c} \copygen \\ \oplus \\ \id \end{array} 
\!\!\!\!\!
\mathrel{;}
\!\!\!\!\! 
 \begin{array}{c} \minonegen \\ \oplus \\ \id \end{array} 
\!\!\!\!\! 
 \mathrel{;}
\!\!\!\!\! 
 \begin{array}{c} \delayopgen \\ \oplus \\ \id \end{array} 
 \!\!\!\!\! 
 \mathrel{;}
\!\!\!\!\! 
 \begin{array}{c} \id \\ \oplus \\ \copygen \end{array} 
  \!\!\!\!\! 
 \mathrel{;}
\!\!\!\!\! 
 \begin{array}{c} \copyopgen \\ \oplus \\ \id \end{array} 
 \!\!\!\!\! 
 \mathrel{;}
\!\!\!\!\! 
 \begin{array}{c} \discardgen \\ \oplus \\ \id \end{array} 
\]
The building blocks come from the signature of an algebraic theory---a
\emph{symmetric monoidal theory} to be exact. The terms of this theory comprise
the morphisms of a \emph{prop}, a symmetric monoidal category in which the
objects are the natural numbers. With an operational semantics suggested by the
above example, the terms can also be considered as a process algebra for signal
flow graphs. The idea of understanding complex systems by ``tearing'' them into
more basic components, ``zooming'' to understand their individual behaviour and
``linking'' to obtain a composite system is at the core of the behavioural
approach in control. The algebra of symmetric monoidal categories
thus seems a good fit for a formal account of these compositional principles.

This work is the first to make this link between monoidal categories and the
behavioural approach to control explicit. Moreover, it is the first to endow
signal flow graphs with their standard systems theoretic semantics in which the
registers---the `$s$' gates---are permitted to hold \emph{arbitrary} values at
the beginning of a computation.  This extended notion of behaviour is not merely
a theoretical curiosity: it gives the class of \emph{complete LTI discrete
dynamical systems}. The interest of systems theorists is due to practical
considerations: physical systems seldom evolve from zero initial conditions.

Although previous work~\cite{BSZ2,BSZ3,Za} made the connection between signal
flow graphs and string diagrams, their operational semantics is more restrictive
than that considered here, considering only trajectories with finite past and
demanding that, initially, all the registers contain the value $0$.  Indeed,
with this restriction, it is not difficult to see that the trajectories
of~\eqref{eq:examplesfg} are those where the output is \emph{the same} as the
input. The input/output behaviour is thus that of a stateless wire.  The
equational presentation in this case is the theory $\ih_{\k[s]}$ of
interacting Hopf algebras \cite{Za}, and indeed, in $\ih_{\k[s]}$:
\begin{equation}\label{eq:exampleproof}
\lower12pt\hbox{$\includegraphics[height=2cm]{pics/exampleproof.pdf}$} 
\end{equation}

Note that~\eqref{eq:exampleproof} is \emph{not sound} for circuits
with our more liberal, operational semantics. Indeed, recall that when the
registers of~\eqref{eq:examplesfg} initially hold values $1$ and $2$, the input
$-1,1,-1,1,\dots$ results in the output $-2,2,-2,2,\dots$. This trajectory is
not permitted by a stateless wire, so $\ih_{k[s]}$ is not sound for reasoning
about LTI systems in general. We have provided a sound
and complete theory to do just that.

In terms of the algebraic semantics, the difference from previous
work is that where there streams were handled with Laurent
(formal power) series, here we use biinfinite streams. These are sequences of
elements of $\k$ that are infinite in the past \emph{as well} as in the
future---that is, elements of $\k^\z$.  Starting with the operational
description, one obtains a biinfinite trajectory by executing circuits forwards
and backwards in time, for some initialisation of the registers. The dynamical
system defined by a signal flow diagram is the set of trajectories obtained by
considering all possible executions from all possible initialisations.  Indeed,
this is the very extension that allows us to discuss non-controllable
behaviours; in~\cite{BSZ,BSZ3,Za,BE} all definable behaviours were controllable. 

An equational theory also requires equations between the terms. We obtain the
equations in two steps. First, we show there is a full, but not faithful,
morphism from the prop $\cospan\mat\pk$ of cospans of matrices over the ring
$\pk$ to the prop $\ltids$ of complete LTI discrete dynamical systems.
Using the presentation of $\cospan\mat\pk$ in~\cite{BSZ2,Za}, the result is a
sound, but not complete, proof system. The second ingredient is restricting our
attention from cospans to corelations. This gives a faithful morphism, allowing
us to present the prop of corelations as a symmetric monoidal theory, and hence
giving a sound and complete proof system for reasoning about LTIs (Theorem
\ref{thm.main}).

The advantages of the string diagram calculus over the traditional matrix
calculus are manifold. The operational semantics make the notation intuitive, as
does the compositional aspect: it is cumbersome to describe connection of
systems using matrices, whereas with string diagrams you just connect the right
terminals. Moreover, the calculus unifies the variety of distinct methods for
representing LTI systems with matrix equations---built from kernel and image
representations~\cite{Wi,Wi3}---into a single framework, heading off
possibilities for ambiguity and confusion.

We hope, however, the greatest advantage will be the way these properties can be
leveraged in analysis of controllability. In Theorem \ref{cor.spanreps}, we
show that in our setting controllability has an elegant structural
characterisation.  Compositionality pays off here, with our proof system giving
a new technique for reasoning about control of compound systems (Proposition
\ref{prop:veryexciting}).  From the systems theoretic point of view, these
results are promising since the compositional, diagrammatic techniques we bring
to the subject seem well-suited to problems such as controllability of
interconnections, of primary interest for multiagent and spatially
interconnected systems~\cite{OFM}.
%
%Summing up, our original technical contributions are:
%\begin{itemize}
%\item a characterisation of the class of LTI systems as a category of corelations of matrices
%\item a presentation of this category of corelations of matrices as a symmetric
%  monoidal theory
%\item an operational semantics that agrees with the standard systems theoretic
%semantics of signal flow graphs
%\item a characterisation of controllability
%\end{itemize}

\section{Linear time-invariant dynamical systems} \label{sec.systems}

Following Willems~\cite{Wi3}, a \define{dynamical system} $(T, W,\bb)$ is: a
\define{time axis} $T$, a \define{signal space} $W$, and a \define{behaviour}
$\bb \subseteq W^T$. We refer to $w \in \bb$ as \define{trajectories}. 

Here we are interested in discrete trajectories that are \define{biinfinite}:
infinite in past and future.  Our time axis is thus the integers $\z$.  Let $\k$
be a field: for concreteness one may take this to be the rationals $\mathbb{Q}$,
the reals $\R$ or the booleans $\z_2$.  The signal space is $\k^d$, where $d$ is
the number of \define{terminals} of a system.  These, in engineering terms, are
the interconnection variables that enable interaction with an environment.

The dynamical systems of concern to us are thus specified by some natural number
$d$ and a subset $\bb$ of $(\k^d)^\z$. The sets $(\k^d)^\z$ are $\k$-vector
spaces, with pointwise addition and scalar multiplication. We restrict attention
to \emph{linear} systems, meaning that $\bb$ is required to be a \emph{$\k$-linear subspace}---i.e.
closed under addition and multiplication by $\k$-scalars---of
$(\k^d)^\z$. 

We partition terminals into a \emph{domain} and \emph{codomain} of $m$ and $n$
terminals respectively, writing $\bb \subseteq (\k^m)^\z \oplus (\k^n)^\z \cong
(\k^d)^\z$.  This may seem artificial, in the sense that the assignment is
arbitrary.  In particular, it is crucial not to confuse the domains (codomains)
with inputs (outputs). In spite of the apparent contrivedness of choosing such a
partition, Willems and others have argued that it is vital for a sound theory of
system \emph{decomposition}; indeed, it enables the ``tearing'' of Willems'
tearing, zooming and linking~\cite{Wi}.
%. We
%use this structure to partition the system into a part we wish to connect to
%another system, and a part which we do not. It is an artifice in the sense that
%we may take any terminal in the domain and move it to the codomain, and vice
%versa. This ability is called compactness for a monoidal category.

Once the domains and codomains have been chosen, systems are linked by
connecting terminals. In models of physical systems this means variable coupling
or sharing; in our discrete setting where behaviours are subsets of a
cartesian product---i.e.\ relations---it amounts to relational composition.
Since behaviours are both relations and linear subspaces, a central underlying
mathematical notion is a linear relation.  

\smallskip
%The next restriction is the property of \define{time-invariance}. 
A behaviour is \define{time-invariant} when for every trajectory $w \in \bb$ and
any fixed $i\in\z$, the trajectory whose value at every time $t\in\z$ is
$w(t+i)$ is also in $\bb$.  
%
Time-invariance brings with it a connection with the algebra of polynomials.
Following the standard approach in control theory, going back to
Rosenbrock~\cite{Ro}, we work with polynomials over an indeterminate $s$ as well
as its formal inverse $s^{-1}$---i.e.\ the elements of the ring
$\pk$.

The indeterminate $s$ acts on a given biinfinite stream $w \in \k^\z$ as a
one-step delay, and $s^{-1}$ as its inverse, a one step anticipation: 
\[ 
  (s\cdot w) (t) \Defeq w(t-1),\quad (s^{-1}\cdot w)(t) \Defeq w(t+1).
\]
We can extend this, in the obvious linear, pointwise manner, to an action of any
polynomial $p\in \pk$ on $w$.  Since $\k^\z$ is a $\k$-vector space, any such
$p$ defines a $\k$-linear map $\k^\z\to \k^\z$ ($w \mapsto p\cdot w$).

Given this, we can view $n\times m$ matrices over $\pk$ as $\k$-linear maps from
$(\k^m)^\z$ to $(\k^n)^\z$. This viewpoint can be explained succinctly as a
functor from the prop $\mat\pk$, defined below, to the category of $\k$-vector
spaces and linear transformations $\vect_\k$.

Recall that a \define{prop} is a strict symmetric monoidal category where the
set of objects is the natural numbers $\nn$, and monoidal product ($\oplus$) on
objects is addition. Homomorphism of props are identity-on-objects strict
symmetric monoidal functors.

\begin{definition}
  The prop $\mat\pk$ has as arrows $m \to n$ the $n\times m$-matrices over
  $\pk$. Composition is matrix multiplication, and the monoidal product of $A$
  and $B$ is $\left[\begin{smallmatrix} A & 0 \\ 0 & B
  \end{smallmatrix}\right]$. The symmetries are permutation matrices.
\end{definition}

The functor of interest
\[
  \vectfun\maps \mat\pk \longrightarrow \vect_\k
\]
takes a natural number $n$ to $(\k^n)^\z$, and an $n\times m$ matrix to the
induced linear transformation $(\k^m)^\z \to (\k^n)^\z$. Note that $\vectfun$ is
faithful.

\smallskip
The final restriction on the set of behaviours is called \emph{completeness},
and is a touch more involved. For $t_0,t_1 \in \z$, $t_0 \le t_1$, write
$w|_{[t_0,t_1]}$ for the restriction of $w: \z \to \k^n$ to the set $[t_0,t_1] =
\{t_0, t_0+1, \dots, t_1\}$. Write  $\bb|_{[t_0,t_1]}$ for the set of the
restrictions of all trajectories $w \in \bb$ to $[t_0,t_1]$.  Then $\bb$ is
\define{complete} when $w|_{[t_0,t_1]} \in \bb|_{[t_0,t_1]}$ for all $t_0,t_1
\in \z$ implies $w \in \bb$. This topological condition is important as it
characterises the linear time-invariant behaviours that are kernels of the
action of $\mat\pk$; see Theorem \ref{thm.kernelreps}.

%We shall introduce one additional structure: a partition of the terminals of a
%dynamical system into two subsets, called a domain and a codomain. This will
%provide us with language to talk about interconnection of dynamical systems.
%We thus make the following definition.

\begin{definition}
  A \define{linear time-invariant (LTI) behaviour} comprises a domain
  $(\k^m)^\z$, a codomain $(\k^n)^\z$, and a subset $\bb \subseteq (\k^m)^\z
  \oplus (\k^n)^\z$ such that $(\z,\k^m \oplus \k^n,\bb)$ is a complete, linear,
  time-invariant dynamical system.
\end{definition}

The algebra of LTI behaviours is captured concisely as a prop.
\begin{proposition} \label{prop.ltidsiswelldefined}
  There exists a prop $\ltids$ 
  % objects the vector spaces of the form $(\k^n)^\z$ for $n \in \nn$, and 
  % morphisms $(\k^n)^\z \to (\k^m)^\z$ the LTIDS with domain
  with morphisms $m \to n$ the LTI behaviours with domain $(\k^m)^\z$ and
  codomain $(\k^n)^\z$. Composition is relational. The monoidal product is
  direct sum.
\end{proposition}

%\subsection{Kernel representations}


The proof of Proposition~\ref{prop.ltidsiswelldefined} relies on \emph{kernel
representations} of LTI  systems.  The following result lets us pass between
behaviours and polynomial matrix algebra.
%\[
%\mathfrak{B}:=\{w:\mathbb{Z} \rightarrow \k^q \mid \mbox{\rm s.t.~} R(\sigma,\sigma^{-1})w=0\}\; .
%\]
%and (for \emph{controllable} behaviours) the \emph{image representation}
%\begin{equation}\label{eq:im}
%w=M(\sigma,\sigma^{-1})\ell\; ,
%\end{equation}
%where $M\in\R^{q\times m}[s,s^{-1}]$, defining the behaviour
%\[
%\mathfrak{B}:=\{w:\mathbb{Z} \rightarrow \R^q \mid  \mbox{\rm exists } \ell:\mathbb{Z}\rightarrow \R^m \mbox{\rm s.t. (\ref{eq:ker}) holds}\}\; ,
%\]}
%
%
\begin{theorem}[Willems {\cite[Theorem 5]{Wi3}}] \label{thm.kernelreps}
  Let $\bb$ be a subset of $(\k^n)^\z$ for some $n \in \mathbb N$. Then $\bb$ is
  an LTI behaviour iff there exists $M \in
  \mat\pk$ such that $\bb = \mathrm{ker}(\vectfun M)$.
\end{theorem}

%Completeness is an important property. An example of a non-complete linear
%time-invariant subspace of $\k^\z$ is the set of all finitely supported
%functions $\z \to \k$.  This is not the kernel of the action of any matrix.

%\smallskip
The prop $\mat \pk$ is equivalent to the category $\fmod\pk$ of
finite dimensional free $\pk$-modules. Since $\fmod R$ over a principal ideal
domain (PID) $R$ has finite colimits \cite{BSZ2}, and $\pk$ is a PID, $\mat \pk$
has finite colimits, and thus it has pushouts.

We can therefore define the prop $\cospan\mat\pk$ where arrows are (isomorphism
classes of) cospans of matrices: arrows $m \to n$ comprise a natural number
$d$ together with a $d\times m$ matrix $A$ and a $d\times n$ matrix $B$; we
write this $m\xrightarrow{A} d \xleftarrow{B}n$. 

We can then extend $\vectfun$ to the functor
\[
  \cospanfun \maps \cospan\mat\pk \longrightarrow \linrel_\k
\]
where on objects $\cospanfun(n)=\vectfun(n)=(\k^n)^\z$, and on arrows
%defined by mapping each cospan to a linear relation:
\[
  m\xrightarrow{A} d \xleftarrow{B}n
\]
maps to
\begin{equation}\label{eq.K}
  \big\{(\mathbf{x},\mathbf{y}) \,\big|\,
  (\vectfun A)\mathbf{x} = (\vectfun B)\mathbf{y}\big\} 
  \subseteq (\k^m)^\z \oplus (\k^n)^\z.
\end{equation}
It is straightforward to prove that this is well defined.
%\begin{remark}\rm
%From a system theory point of view, the definition~\eqref{eq.K}, can be interpreted as associating with $R_1(\sigma,\sigma^{-1}), R_2(\sigma,\sigma^{-1})$, where 
%$R_i\in\R^{p\times n_i}[s,s^{-1}]$, the set of trajectories  
%\begin{multline}\label{eq:mycospan}
%\mathcal{Z}_{R_1,R_2}:= \\
%\{ w:\mathbb{Z}\rightarrow \k^{n_1+n_2} \mid 
%w=(w_1,w_2),\,
%R_1(\sigma,\sigma^{-1})w_1 = R_2(\sigma,\sigma^{-1})w_2 \}
%\end{multline}
%where $w=(w_1,w_2)$ is a terminal partition.
%\end{remark}
\begin{proposition}\label{prop.funct}
$\cospanfun$ is a functor.
\end{proposition}
\begin{proof}
Identities are clearly preserved; it suffices to show that composition is too.
Consider the diagram below, where the pushout is calculated in $\mat\pk$.
\[
\xymatrix{
  {}\ar[dr]_{A_1} & & \ar[dl]^{B_1}  
  \ar[dr]_{A_2} & & \ar[dl]^{B_2} {} 
 \\
& \ar[dr]_{C} & & \ar[dl]^{D}  \\
& & {\save*!<0cm,-.5cm>[dl]@^{|-}\restore}
}
\]
To show that $\cospanfun$ preserves composition we must verify that 
\[
  \{(\mathbf{x},\mathbf{y})\,|\, \theta CA_1\mathbf{x} = \theta DB_2\mathbf{y}\} =
  \{(\mathbf{x},\mathbf{z})\,|\, \theta A_1\mathbf{x} = \theta B_1\mathbf{z}\} ; 
  \{(\mathbf{z},\mathbf{y})\,|\, \theta A_2\mathbf{z} = \theta B_2\mathbf{y}\}.
\]
The inclusion $\subseteq$ follows from properties of pushouts in $\mat\pk$ (see
\cite[Proposition 5.7]{BSZ2}).  To see $\supseteq$, we need to show that if there
exists $\mathbf{z}$ such that $\theta A_1\mathbf{x} = \theta B_1\mathbf{z}$ and
$\theta A_2\mathbf{z} = \theta B_2\mathbf{y}$, then $\theta CA_1\mathbf{x} =
\theta DB_2\mathbf{y}$.  But $\theta CA_1\mathbf{x}=\theta CB_1\mathbf{z}=\theta
DA_2\mathbf{z}=\theta DB_2\mathbf{y}$.
\end{proof}



%Paolo's original remark
%\begin{remark}\rm
%\textcolor{red}{The result of Proposition \ref{prop:Kfunct} can be interpreted from a system theory point of view as identifying with a terminal decomposition $w=(w_1,w_2)$ of the variables of a LTI behaviour $\bb$ and associated kernel representation
%\begin{equation}\label{eq:ker}
%R_1(\sigma,\sigma^{-1})w_1=R_2(\sigma,\sigma^{-1})w_2\; ,
%\end{equation} 
%where $R_i\in\R^{p\times n_i}[s,s^{-1}]$, the set of trajectories 
%\begin{equation}\label{eq:mycospan}
%\mathcal{Z}_{R_1,R_2}:=\{z:\mathbb{Z}\rightarrow \R^p \mid \exists w_i:\mathbb{Z}\rightarrow \R^{n_i}\mbox{~s.t.~} z=R_i(\sigma,\sigma^{-1})w_i,~i=1,2\}\; .
%\end{equation}}
%\end{remark}

Rephrasing the definition of $\cospanfun$ on morphisms~\eqref{eq.K}, the
behaviour consists of those $(\mathbf{x},\mathbf{y})$ that satisfy
\[
  \vectfun\left[\begin{array}{cc} A & -B\end{array}\right]
  \left[\begin{array}{c}\mathbf{x}\\\mathbf{y}\end{array}\right] 
  = \mathbf{0},
\]
so one may say---ignoring for a moment the terminal domain/codomain assignment---that 
\[ 
  \cospanfun (\xrightarrow{A}\xleftarrow{B}) = \ker \vectfun\left[
  \begin{array}{cc} A & -B\end{array}\right].
\]

With this observation, as a consequence of Theorem~\ref{thm.kernelreps},
$\cospanfun$ has as its image (essentially) the prop $\ltids$. This proves
Proposition \ref{prop.ltidsiswelldefined}. We may thus consider $\cospanfun$ a
functor onto the codomain $\ltids$; denote this corestriction $\cospanfunrest$. We thus have a full functor:
\[
  \cospanfunrest \maps \cospan\mat\pk \longrightarrow \ltids.
\]

\begin{remark}\label{rmk:faithfulness}
It is important for the sequel to note that $\cospanfunrest$ is \emph{not} faithful.
For instance, $\cospanfunrest(1\xrightarrow{[1]}1\xleftarrow{[1]}1) =
\cospanfunrest(1\xrightarrow{ \left[\begin{smallmatrix} 1\\ 0\end{smallmatrix}\right]
}2 \xleftarrow{ \left[\begin{smallmatrix} 1\\ 0\end{smallmatrix}\right]} 1)$, yet
the cospans are not isomorphic. The task of the next section is to develop a
setting where these cospans are nonetheless \emph{equivalent}.
\end{remark}

\section{Presentation of $\ltids$} \label{sec.diagrams}
Recall that a \define{symmetric monoidal theory} (SMT) is a presentation of a prop: a pair
$(\Sigma,E)$ where $\Sigma$ is a set of \define{generators} $\sigma\colon m\to
n$, where $m$ is the \define{arity} and $n$ the \define{coarity}. A
$\Sigma$-term is a obtained from $\Sigma$, identity $\idn\colon 1\to 1$ and
symmetry $\tw\colon 2\to 2$ by composition and monoidal product, according to
the grammar
\[
  \tm\ ::=\ \sigma\ |\ \idn\ |\ \tw\ |\ \tm\mathrel{;}\tm\ |\ \tm\oplus \tm 
\]
where $\mathrel{;}$ and $\oplus$ satisfy the standard typing discipline that
keeps track of the domains (arities) and codomains (coarities)
\[
\frac{\tm: m\to d \quad \tm': d\to n}
{\tm\mathrel{;}\tm': m\to n}
\quad
\frac{\tm: m\to n \quad \tm': m'\to n'}
{\tm\oplus \tm': m+m'\to n+n'}
\]
The second component $E$ of an SMT is a set of \define{equations}, where an
equation is a pair $(\tm,\mu)$ of $\Sigma$-terms with compatible types, i.e.
$\tm,\mu\colon m\to n$ for some $m,n\in\mathbb{N}$.

Given an SMT $(\Sigma,E)$, the prop $\mathbf{S}_{(\Sigma,E)}$ has as arrows the
$\Sigma$-terms quotiented by the smallest congruence that includes the laws of
symmetric monoidal categories and equations $E$. We sometimes abuse
notation by referring to $\mathbf{S}_{(\Sigma,E)}$ as an SMT. Given an arbitrary
prop $\mathbb{X}$, a \define{presentation} of $\mathbb{X}$ is an SMT
$(\Sigma,E)$ s.t.\ $\mathbb{X} \cong \mathbf{S}_{(\Sigma,E)}$.

In this section we give a presentation of $\ltids$ as an SMT. This means 
that (\emph{i}) we obtain a syntax---conveniently expressed using string
diagrams---for specifying every LTI behaviour, and (\emph{ii}) a sound and
complete equational theory for reasoning about them.

\subsection{Syntax}
We start by describing the graphical syntax of dynamical systems, the arrows of
the category $\syntax = \mathbf{S}_{(\Sigma,\varnothing)}$, where $\Sigma$ is
the set of generators:
\begin{multline}\label{eq:generators}
\{
\addgen,
\zerogen,
\copygen,
\discardgen,
\delaygen, 
\addopgen,
\zeroopgen,
\copyopgen,
\discardopgen,
\delayopgen
\} \\
\cup \{ \scalargen \mid a\in\k \,\} \cup \{ \scalaropgen \mid a\in\k \,\}
\end{multline}
For each generator, we give its denotational semantics, an LTI behaviour,
thereby defining a prop morphism $\llbracket - \rrbracket: \syntax \to\ltids$.
\[
\addgen \!\!\mapsto \{\,( \left( 
  {\begin{smallmatrix}\tau \\ \upsilon\end{smallmatrix}} \right),\, \tau+\upsilon) \mid \tau,\upsilon \in \k^\z \,\}\maps 2\to 1
\]
\[
\quad
\zerogen \!\!\mapsto
\{\,((),0)\,\} \subseteq \k^\z\maps 0\to 1
\]
\[
\copygen \!\!\mapsto 
\{\, (\tau, \left( \begin{smallmatrix} \tau \\ \tau\end{smallmatrix} \right)) \mid \tau\in \k^\z \,\}\maps1\to 2
\]
\[
\quad
\discardgen \!\!\mapsto
\{\,(\tau,()) \mid \tau \in \k^\z\,\} \maps 1\to 0
\]
\[
\scalargen  \!\!\mapsto
\{\, (\tau, a\cdot\tau) \mid \tau\in\k^\z \, \} \maps 1 \to 1 \quad (a\in\k)
\]
\[
\ 
\delaygen \!\!\mapsto
\{\, (\tau, s\cdot\tau) \mid \tau\in\k^\z\,\} \maps 1 \to 1
\]

% In Fig.~\ref{fig:generators} we introduce half of the
%generators and their denotations.
%\begin{figure*}[ht]
%\[
%\addgen \!\!\mapsto \{\,( \left( 
%  {\begin{smallmatrix}\tau \\ \upsilon\end{smallmatrix}} \right),\, \tau+\upsilon) \mid \tau,\upsilon \in \k^\z \,\}\maps 2\to 1
%\]
%\[
%\quad
%\zerogen \!\!\mapsto
%\{\,((),0)\,\} \subseteq \k^\z\maps 0\to 1
%\]
%\[
%\copygen \!\!\mapsto 
%\{\, (\tau, \left( \begin{smallmatrix} \tau \\ \tau\end{smallmatrix} \right)) \mid \tau\in \k^\z \,\}\maps1\to 2
%\]
%\[
%\quad
%\discardgen \!\!\mapsto
%\{\,(\tau,()) \mid \tau \in \k^\z\,\} \maps 1\to 0
%\]
%\[
%\scalargen  \!\!\mapsto
%\{\, (\tau, a\cdot\tau) \mid \tau\in\k^\z \, \} \maps 1 \to 1 \quad (a\in\k)
%\]
%\[
%\ 
%\delaygen \!\!\mapsto
%\{\, (\tau, s\cdot\tau) \mid \tau\in\k^\z\,\} \maps 1 \to 1
%\]
%\caption{Generators and their denotations in $\ltids$. \label{fig:generators}}
%\end{figure*}
\noindent 
The denotations of the mirror image generators are the opposite relations.
Parenthetically, we note that a finite set of generators is possible
over a finite field, or the field $\mathbb{Q}$ of rationals, \emph{cf.}\
\S\ref{subsec:eqhopf}.

The following result guarantees that the syntax is fit for purpose: every
behaviour in $\ltids$ has a syntactic representation in $\syntax$.
\begin{proposition}\label{prop.syntaxfull}
  $\llbracket - \rrbracket: \syntax\to\ltids$ is full.
\end{proposition}
\begin{proof}
  The fact that $\llbracket - \rrbracket$ is a prop morphism is immediate since
  $\mathbb{S}$ is free on the SMT $(\Sigma,\varnothing)$ with no
  equations.  Fullness follows from the fact that $\llbracket - \rrbracket$
  factors as the composite of two full functors:
  \[
    \xymatrixrowsep{2pc}
    \xymatrix{
      \syntax \ar[d] \ar[dr]^{\llbracket-\rrbracket} \\
      \cospan\mat\pk \ar[r]_-{\cospanfunrest} & \ltids
    }
  \]
  The functor $\cospanfunrest$ is full by definition. The existence and fullness
  of the functor $\mathbb{S}\to\cospan\pk$ follows from \cite[Theorem 3.41]{Za}. We
  give details in the next two subsections.
\end{proof}

Having defined the syntactic prop $\syntax$ capable of representing every
behaviour in $\ltids$, our task for this section is to identify an equational
theory that \emph{characterises} equivalent representations in $\ltids$: i.e.
one that is sound and complete.  The first step is to use the existence of
$\cospanfun$: with the results of~\cite{BSZ2,Za} we can obtain a presentation
for $\cospan\mat\pk$. This is explained in the next subsection, where we present
$\mat\pk$ and $\cospan\mat\pk$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Presentations of $\mat\pk$ and $\cospan\mat\pk$}\label{subsec:eqhopf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To obtain a presentation of $\mat\pk$ as an SMT we only require 
some of the generators:
\[
  \{
\addgen,
\zerogen,
\copygen,
\discardgen,
\delaygen,
\delayopgen\}
\cup \{ \scalargen \mid a\in\k \,\}
\]
and the
% of
%Fig.\ref{fig:generators}, $\lower6pt\hbox{$\includegraphics[height=0.6cm]{pics/delayop.pdf}$}$
%\[
%\{
%\lower8pt\hbox{$\includegraphics[height=0.7cm]{pics/add.pdf}$},
%\lower5pt\hbox{$\includegraphics[height=0.5cm]{pics/zero.pdf}$},
%\lower8pt\hbox{$\includegraphics[height=0.7cm]{pics/copy.pdf}$},
%\lower5pt\hbox{$\includegraphics[height=0.5cm]{pics/discard.pdf}$},
%\lower6pt\hbox{$\includegraphics[height=0.6cm]{pics/delay.pdf}$},
%\lower6pt\hbox{$\includegraphics[height=0.6cm]{pics/delayop.pdf}$}
% \}
% \cup
% \{
% \lower6pt\hbox{$\includegraphics[height=0.6cm]{pics/scalar.pdf}$}
%\mid a \in \k\,
%\}
%\]
%and the 
following equations. %, see \cite{BSZ2,Za}. 
First, the white and the black structure forms a (bicommutative) bimonoid:
\[
\includegraphics[width=.4\textwidth]{pics/bimonoid.pdf}
\]
Next, the formal indeterminate $s$ is compatible with the bimonoid structure
and has its mirror image as a formal inverse.
\[
\includegraphics[height=3.1cm]{pics/s.pdf}
\]
Finally, we insist that the algebra of $\k$ be compatible with the bimonoid structure and commute with $s$.
%The equations below, in which $a,b\in\k$, are redundant if $\k=\mathbb{Q}$; instead,
%one needs an additional generator, the antipode, identified with the scalar
%$-1$. The details are not important; we merely mention that in other work we
%have used  
%\lower6pt\hbox{$\includegraphics[height=0.6cm]{pics/antipode.pdf}$}
%to represent
%\lower6pt\hbox{$\includegraphics[height=0.6cm]{pics/minusone.pdf}$}, and that
%henceforward we adopt this convention.
\[
\includegraphics[height=5.3cm]{pics/scalars.pdf}
\]
The three sets of equations above form the theory of Hopf algebras. Write
$\ha_\pk$ for the prop induced by the SMT consisting of the equations
above. The following follows from~\cite[Proposition~3.9]{Za}.
\begin{proposition}
$\mat\pk\cong\ha_\pk$.
\end{proposition}

Arrows of $\mat\pk$ are matrices with polynomial entries, but it may not be
 obvious to the reader how polynomials arise with the string
diagrammatic syntax. We illustrate this below.

\begin{example}\label{rem:polys}
Any polynomial $p=\sum_{i=u}^v a_i s^i$, where $u\leq v\in\z$ and with
coefficients $a_i\in\k$, can be written graphically using the building blocks of
$\mathbb{HA}_\pk$. Rather than giving a tedious formal construction, we
illustrate this with an example for $\k=\R$. A term for $3s^{-3}-\pi
s^{-1}+s^{2}$ is:
\[
\includegraphics[height=1.7cm]{pics/polyex.pdf}
\]

As an arrow $1 \to 1$ in $\mat\pr$, the above term represents a $1\times
1$-matrix over $\pr$. To demonstrate how higher-dimensional matrices can be
written, we also give a term for the $2 \times 2$-matrix $\begin{bmatrix} 2 & 3s
  \\ s^{-1} & s+1 \end{bmatrix}$:
\[
\includegraphics[height=3cm]{pics/exmatrix.pdf}
\]
The above examples are intended to be suggestive of a normal form for terms in
$\ha_\pk$; for further details see \cite{Za}.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


To obtain the equational theory of $\cospan\mat \pk$ we need the full set of
generators~\eqref{eq:generators}, along with the equations of $\ha_\pk$, their mirror images, and the
following
\[
\includegraphics[height=4cm]{pics/cospan.pdf}
\]
where $p$ ranges over the nonzero elements of $\pk$ (see
Example~\ref{rem:polys}). Note that in the second equation on the right-hand
side, we use the so-called `empty diagram', or blank space, to represent the
identity map on the monoidal unit, 0. 

The equations of $\ha_\pk$ ensure the generators of $\ha_\pk$ behave as
morphisms in $\mat\pk$, while their mirror images ensure the remaining
generators behave as morphisms in the opposite category $\mat\pk^{\mathrm{op}}$.
The additional equations above govern the interaction between these two sets of
generators, axiomatising pushouts in $\mat\pk$.
%
Let $\ihcsp$ denote the resulting SMT.
The procedure for obtaining the equations from a distributive law of props 
is explained in~\cite[\S{3.3}]{Za}.
\begin{proposition}[Zanasi~{\cite[Theorem~3.41]{Za}}]\label{prop:cospanpresentation}
\[\cospan \mat\pk \cong \ihcsp.\]
\end{proposition}

Using Proposition~\ref{prop:cospanpresentation} and the existence of $\cospanfunrest$,
%\[
%  \cospanfun \maps \cospan\mat\pk \longrightarrow \ltids
%\]
the equational theory of $\ihcsp$ is a sound proof system for reasoning about
$\ltids$. Due to the fact that $\cospanfunrest$ is not faithful (see
Remark~\ref{rmk:faithfulness}), however, the system is not complete. Achieving
completeness is our task for the remainder of this section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Corelations in $\mat\pk$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this subsection we identify a factorisation system in $\mat\pk$, and show
that the induced prop $\corel\mat\pk$ of corelations is isomorphic to $\ltids$.
We then give a presentation of $\corel\mat\pk$ and arrive at a sound and 
complete equational theory for $\ltids$.

Recall that split mono is a morphism $m\colon X\to Y$ such that there exists
$m'\colon Y\to X$ with $m'm=\idn_X$. Every morphism in $\mat\pk$ admits a
factorisation into an epi followed by a split mono. 

\begin{proposition}\label{prop.matfactorisation}
  Every morphism $R \in \mat\pk$ can be factored as $R = BA$, where $A$ is an
  epi and $B$ is a split mono.
\end{proposition}
\begin{proof}
%We use the Smith normal form~\cite[Section~6.3]{Ka}. 
%
Given any matrix $R$, the Smith normal form~\cite[Section~6.3]{Ka}  gives
  us  $R = VDU$, where $U$ and $V$ are invertible, and $D$
  is diagonal. In graphical notation we can write it thus:
  \[
\includegraphics[height=1.1cm]{pics/smith1.pdf}
  \]
  This implies we may write it as $R = U';D';V'$, where $U'$ is a split
  epimorphism, $D'$ diagonal of full rank, and $V'$ a split monomorphism.
  Explicitly, the construction is given by
  \[
\includegraphics[height=1cm]{pics/uv.pdf}
  \]
  Recall that $\pk$ is a PID, so the full rank diagonal matrix $D'$ is epi. It
  can be checked that $R = V'(D'U')$ is an epi-split mono factorisation. 
%In Appendix~\ref{app.prop.matfactorisation}.
\end{proof}
Recalling the argument in Example \ref{ex.factsysts},
Proposition~\ref{prop.matfactorisation} yields:

\begin{corollary} \label{cor.episplitmono}
  Let $\mathcal E$ be the subcategory of epis and
  $\mathcal M$ the subcategory of split monos. The pair $(\mathcal
  E,\mathcal M)$ is a factorisation system on $\mat\pk$, with $\mathcal M$
  stable under pushout.
\end{corollary}
%\begin{proof}
%In Appendix~\ref{app.cor.episplitmono}. See Appendix~\ref{app.factorisation} for background.
%\end{proof}

%The remainder of this section is devoted to proving this proposition. To begin,
%note that both these subcategories contain all isomorphisms of $\mat\pk$. Next,
%observe that every matrix can indeed be factored as required.

%It remains to prove property (iii): functoriality. We derive this from the
%functoriality of a factorisation system on $\vect$. For this, we need the
%following fundamental result. 

%\begin{proposition}[Willems {\cite[p.565]{Wi3}}] \label{prop.magic}
%  Let $M,N$ be matrices in $\mat\pk$. Then $\ker \vectfun M \subseteq \ker \vectfun N$ if and
%  only if there exists a matrix $X$ such that $M;X = N$.
%\end{proposition}

%We also use the following straightforward observation.
%\begin{lemma}[Willems {\cite[p.576]{Wi3}}] \label{lem.surjective}
%  Let $p \in \pk$. Then the linear map $\vectfun(p): \k^\z \to \k^\z$ is an
%  epimorphism.
%\end{lemma}

%\begin{proof}[Proposition \ref{prop.fact}]
%  Let $R \in \mat\pk$ factor as $R= (U';D');V'$ as above. Note that as $U'$ and
%  $V'$ are split epi and split mono respectively, $\vectfun U'$ and $\vectfun V'$ are
%  too, and hence a fortiori epi and mono respectively.  Moreover, $\vectfun D'$ is
%  epi as it acts streamwise on $(k^n)^\z$, and by Lemma \ref{lem.surjective} the
%  action on each stream is a surjective linear map. Thus $\vectfun R = \vectfun
%  (U';D');\vectfun V'$ is an epi-mono factorisation of $\vectfun R$ in Vect. 
%
%  Now $\vect$ is an abelian category, and so has an epi-mono factorisation
%  system \cite{ML}. We will use Proposition \ref{prop.magic} to show the
%  functoriality of the epi-mono factorisation system in $\vect$ implies the
%  functoriality of the epi-splitmono system in $\mat\pk$. To that end, suppose we
%  have matrices $R$, $R'$, $A$, $B$ such that $R;B = A;R'$ and such that we have
%  epi-splitmono factorisations $R = E;M$ and $R' = E';M'$. Then $\vectfun R = \vectfun
 % E;\vectfun M$, $\vectfun R' = \vectfun E';\vectfun M'$ are epi-mono factorisations in
 % $\vect$, so there exists a unique linear map $\xi$ such that
 % \begin{equation} \label{eq.funvect}
 %   \begin{aligned}
 %     \xymatrixcolsep{3pc}
 %     \xymatrixrowsep{3pc}
 %     \xymatrix{
%	\ar[r]^{\vectfun E} \ar[d]_{\vectfun A} & \ar[r]^{\vectfun M}
%	\ar@{.>}[d]^{\exists! \xi} &  \ar[d]^{\vectfun B} \\
%	\ar[r]_{\vectfun E'}& \ar[r]_{\vectfun M'} & 
%      }
%    \end{aligned}
%  \end{equation}
%  commutes. This implies $\ker \vectfun E$ is contained in $\ker \vectfun A; \vectfun
%  E'$, so Proposition \ref{prop.magic} implies we have a matrix $X$ such that $E;X
%  = A;E'$. As $E;M;B = R;B = A;R'= A;E';M'$, this further implies that $E;M;B =
%  E;X;M'$, and as $E$ is epi this gives $M;B = X;M'$. Thus $X$ makes the
%  functoriality diagram commute.
%
%  Since $\xi$ is the unique such map that makes (\ref{eq.funvect}) commute, we
%  then have $\theta X = \xi$. By the faithfulness of $\theta$, $X$ is then unique.
%  This proves functoriality.
%\end{proof}


\begin{definition}
  The prop $\corel \mat \pk$ has as morphisms equivalence classes of
  jointly epic cospans in $\mat\pk$.  
  %Composition is given by the jointly epic part of the pushout.
\end{definition}

We have a full morphism
\[
  F\maps \cospan \mat \pk \longrightarrow \corel \mat \pk
\]
mapping a cospan to its jointly epic counterpart given by the
 factorisation system. %We will show that 
Then $\cospanfunrest$
factors through $F$ as follows:
\[
  \xymatrixrowsep{2pc}
  \xymatrix{
    \cospan\mat\pk \ar[d]_F \ar[dr]^{\cospanfunrest} \\
    \corel\mat\pk \ar[r]_-{\Phi} & \ltids
  }
\]
The morphism $\Phi$ along the base of this triangle is an isomorphism of props, 
and this is our main technical result, Theorem~\ref{thm.main}. The proof relies
on the following beautiful result of systems theory.

\begin{proposition}[Willems {\cite[p.565]{Wi3}}] \label{prop.magic}
  Let $M,N$ be matrices over $\pk$. Then $\ker \vectfun M \subseteq \ker \vectfun N$ iff $\exists$ a matrix $X$ s.t.\ $XM = N$.
\end{proposition}

Further details and a brief history of the above proposition can be found in
Schumacher \cite[pp.7--9]{Sc}. 

\begin{theorem}\label{thm.main}
  There is an isomorphism of props 
  \[
    \Phi\maps \corel\mat\pk \longrightarrow \ltids
  \]
  taking a corelation $\xrightarrow{A}\xleftarrow{B}$ 
  to $\cospanfunrest(\xrightarrow{A}\xleftarrow{B}) = {\ker\theta [A \ -B]}$.
  %mapping each natural number $n$ to the space $(\k^n)^\z$ and each corelation
  %\[
  %  n \stackrel{f}\longrightarrow a \stackrel{g}\longleftarrow m
  %\]
  %to the difference kernel $\ker\vectfun (f\ -g)$.
\end{theorem}
\begin{proof}
  For functoriality, start from $\vectfun\maps\mat\pk \to
  \vect$. Now (i) $\vect$ has an epi-mono factorisation system, (ii)
  $\vectfun$ maps epis to epis and (iii) split monos to monos, so $\vectfun$
  preserves factorisations. Since it is a corollary of Proposition \ref{prop.funct}
  that $\theta$ preserves colimits, it follows that $\vectfun$ extends to
   $\Psi\maps\corel\mat\pk \to \corel\vect$. But $\corel\vect$ is
   isomorphic to $\linrel$ (see \textsection\ref{ssec.linrel}). 
  By Theorem \ref{thm.kernelreps}, the image of $\Psi$ is $\ltids$, and taking
  the corestriction to $\ltids$ gives us precisely $\Phi$, which is therefore a full
  morphism of props.

  As corelations $n \to m$ are in one-to-one correspondence with epis out of
  $n+m$, to prove faithfulness it suffices to prove that if two epis $R$ and $S$
  with the same domain have the same kernel, then there exists an invertible
  matrix $U$ such that $UR =S$. This is immediate from
  Proposition~\ref{prop.magic}: if $\ker R= \ker S$, then we can find $U, V$
  such that $UR = S$ and $VS = R$. Since $R$ is an epimorphism, and since $VUR =
  VS = R$, we have that $VU=1$ and similarly $UV =1$. This proves that any two
  corelations with the same image are isomorphic, and so $\Phi$ is full and
  faithful.  
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Presentation of $\corel\mat\pk$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Thanks to Theorem~\ref{thm.main}, the task of obtaining a presentation of $\ltids$
is that of obtaining one for $\corel\mat\pk$. 
%One way to do this is to focus on the full morphism $F$ from cospans to corelations.
%\[
%  \cospan \mat \pk \longrightarrow \corel \mat \pk.
%\]
%
To do this, we start with the presentation
$\mathbb{IH}^{\textsf{Csp}}$ for $\cospan\mat\pk$ of \S\ref{subsec:eqhopf}; the task of
this section is to identify the additional equations that equate
exactly those cospans that map via $F$ to the same corelation.
%; i.e. those cospans $c,c'$ for which $Fc=Fc'$.  

%As such, our task 
%now is to understand the conditions under which two cospans have the same 
%jointly epic parts.

In fact, only one new equation is required, the `white bone' or `extra' law:
\begin{equation}\label{eq.whitebone}
  \lower8pt\hbox{$\includegraphics[height=.7cm]{pics/wbone.pdf}$}\qquad\qquad
\end{equation}
where we have carefully drawn the empty diagram to the right of the equality symbol.
Expressed in terms of cospans, equation \eqref{eq.whitebone} asserts that  
$0\rightarrow 1\leftarrow 0$ and $0\rightarrow 0\leftarrow 0$ are identified:
indeed, the two clearly yield the same corelation. The intuition here is that
cospans $X \xrightarrow{i} S \xleftarrow{j} Y$ map to the same corelation if
their respective copairings $[i,j]\maps X+Y \to S$ have the same jointly epic
parts.  More colloquially, this allows us to `discard' any part of the cospan
that is not connected to the terminals. This is precisely what equation
\eqref{eq.whitebone} represents. Further details on this viewpoint can be found
in \cite{CF}. 
%

Let $\mathbb{IH}^{\mathrm{Cor}}$ be the SMT obtained
from the equations of $\mathbb{IH}^{\mathrm{Csp}}$ together with equation~\eqref{eq.whitebone}.
\begin{theorem}
$\corel \mat \pk \cong \mathbb{IH}^{\mathrm{Cor}}$.
\end{theorem}
\begin{proof}
Since equation~\eqref{eq.whitebone} holds in $\corel\mat\pk$, we have
a full morphism $\ihcor \to \corel\mat\pk$; it remains
to show that it is faithful. It clearly suffices to show that in the equational
theory $\ihcor$ one can prove that every cospan is equal
to its corelation. Suppose then that $m \xrightarrow{A} k \xleftarrow{B} n$ is a cospan
and $m \xrightarrow{A'} k' \xleftarrow{B'} n$ its corelation. Then, by definition, 
there exists a split mono $M\maps k'\to k$ such that $MA'=A$ and $MB'=B$. Moreover, by the construction
of the epi-split mono factorisation in $\mat\pk$, $M$ is of the form $\lower10pt\hbox{$\includegraphics[height=1cm]{pics/m.pdf}$}$ where $M'\maps k\to k$ is invertible. We can now give the derivation
in $\mathbb{IH}^{\mathrm{Cor}}$:
\begin{align*}
\lower7pt\hbox{$\includegraphics[height=.7cm]{pics/cospan1.pdf}$} \quad
&\stackrel{\ihcsp}{=} \quad
\lower10pt\hbox{$\includegraphics[height=1cm]{pics/cospan2.pdf}$} \\
\quad &\stackrel{\ihcsp}{=} \qquad 
\lower8pt\hbox{$\includegraphics[height=.8cm]{pics/cospan3.pdf}$} \\
&\stackrel{\eqref{eq.whitebone}}{=} \qquad\qquad
\lower7pt\hbox{$\includegraphics[height=.7cm]{pics/cospan4.pdf}$}
\end{align*}
\end{proof}


%The new construction rule we have for axioms is that we may replace split-monic
%matrices followed by a cozero with just a cozero. Split monic matrices are just
%the tensor of identity and zero matrices followed by an invertible matrix. The
%rules to see that an invertible matrix followed by a cozero is equal to a cozero
%already exist. So the only additional rule is that a zero followed by a cozero
%is a $0 \to 0$ cozero: ie. nothing. This is the bone law.
%\[
%  \includegraphics[height=.7cm]{pics/wbone.pdf}
%\]
%For the reader's convenience, we collect all the equations in Appendix~\ref{app.equations}%
%
%Thus a cospan may be reduced to its jointly epic part by repeated application of
%the usual laws and the white bone law.  Moreover, two cospans then have the same
%jointly epic part if and only if the are the same up to such applications.

We therefore have a sound and complete equational theory capable of representing
all LTI systems, and also a normal form for each LTI system: every such system
can be written, in an essentially unique way, as a jointly epic cospan of terms
in $\ha_\pk$ in normal form. We summarise the axioms in Figure
\ref{fig.ihcoraxioms}.

\begin{figure} 
  \includegraphics[width=.9\textwidth]{pics/ihcorfixed.pdf}
  \caption{The axioms of $\ihcor$.}\label{fig.ihcoraxioms}
\end{figure}

\begin{remark} \label{rmk.omittedaxs}
 $\ihcor$ %(summarised in Appendix~\ref{app.equations}) 
 can also be described as having the equations of $\ih_\pk$ (see \cite{BSZ2,Za}), but
\emph{without} $\!\!\lower4pt\hbox{$\includegraphics[height=.5cm]{pics/badequation.pdf}$}$,
$\!\lower7pt\hbox{$\includegraphics[height=.8cm]{pics/badequation2.pdf}$}$,
and $\!\lower7pt\hbox{$\includegraphics[height=.8cm]{pics/badequation3.pdf}$}$.
Our results generalise; given any PID $R$ we have (informally speaking):
\begin{align*}
  \ihcor_R &= \ih_R -
  \left\{
    \begin{array}{c}
      \lower6pt\hbox{$\includegraphics[height=.6cm]{pics/badequationgen.pdf}$}, \\
      \lower7pt\hbox{$\includegraphics[height=.7cm]{pics/badequation2gen.pdf}$},\\
      \lower7pt\hbox{$\includegraphics[height=.7cm]{pics/badequation3gen.pdf}$}
    \end{array}
    \middle|\,r\neq 0\in R\,
  \right\} \\
  &\cong \corel \mat R
\end{align*}
and, because of the transpose duality of matrices:
\begin{align*}
  \ih^{\mathrm{Rel}}_R &= \ih_R - 
  \left\{
    \begin{array}{c} 
      \lower6pt\hbox{$\includegraphics[height=.6cm]{pics/goodequationgen.pdf}$},\\
      \lower7pt\hbox{$\includegraphics[height=.7cm]{pics/goodequation2gen.pdf}$}, \\
      \lower7pt\hbox{$\includegraphics[height=.7cm]{pics/goodequation3gen.pdf}$}
    \end{array}
    \middle|\,r\neq 0\in R\,
  \right\}\\
  &\cong \mathrm{Rel} \mat R.
\end{align*}
\end{remark}

\begin{remark}\label{rmk:splus1}
The omitted equations each associate a cospan
with a span that, in terms of behaviour, has the effect of passing to a 
sub-behaviour\footnote{In fact the `largest controllable sub-behaviour' of the system. We explore 
controllability in Section \ref{sec.control}.}.
Often this is a strict sub-behaviour, hence the failure of soundness of $\ih$ identified in the introduction. 

For example, consider the system $\mathcal B$ represented by the cospan
\[
\lower6pt\hbox{$\includegraphics[height=.8cm]{pics/splus1.pdf}$}
\]
We met this system in the introduction; indeed the following
derivation can be performed in $\ihcor$:
\[
\includegraphics[height=1.3cm]{pics/firstequality.pdf}
\]
 The trajectories are $w= (w_1, w_2) \in \k^\z \oplus \k^\z$ where
$(s+1)\cdot w_1= (s+1)\cdot w_2$; that is, they satisfy the difference equation
\[ 
w_1(t-1)+w_1(t)-w_2(t-1)-w_2(t)=0.  
\]
As we saw in the introduction, however, an equation of $\ih_{\k[s]}$ omitted from
$\ihcor_{\k[s,s^{-1}]}$ equates this to the identity system $\id$, with the
identity behaviour those sequences of the form $w = (w_1,w_1) \in \k^\z \oplus
\k^\z$.  The identity behaviour is strictly smaller than $\mathcal B$; e.g., $\mathcal B$ contains $(w_1,w_2)$ with $w_1(t) = (-1)^t$
and $w_2(t) =0$.
\end{remark}

The similarity between our equational presentations of $\ihcor_{\k[s,s^{-1}]}$
and that of $\ih_{\k[s]}$  given in~\cite{BSZ,BSZ3} is remarkable, considering
the differences between the intended semantics of signal flow graphs that string
diagrams in those theories represent, as well as the underlying mathematics of
streams, which for us are elements of the $\k$ vector space $\k^{\mathbb{Z}}$
and in~\cite{BSZ,BSZ3} are Laurent series. We contend that this is evidence of
the \emph{robustness} of the algebraic approach: the equational account of how
various components of signal flow graphs interact is, in a sense, a higher-level
specification than the technical details of the underlying mathematical
formalisations.

\section{Operational semantics} \label{sec.opsem}

In this section we relate the denotational account given in previous sections
with an operational view. 

Operational semantics is given to $\Sigma^*$-terms---that is, to arrows of the
prop $\syntax^*=\mathbf{S}_{(\Sigma^*,\varnothing)}$---where $\Sigma^*$ is
obtained from set of generators in~\eqref{eq:generators} by replacing the formal
variables $s$ and $s^{-1}$ with a family of registers, indexed by the scalars of
$\k$:
\[
\delaygen \leadsto \{ \delaygenk | \, k\in\k \}
\]
\[
\delayopgen \leadsto \{ \delayopgenk |\, k\in\k \}
\]
The idea is that at any time during a computation the register holds the signal
it has received on the previous `clock-tick'. There are no equations, apart from
the laws of symmetric monoidal categories.

Next we introduce the structural rules: the transition relations that occur at
each clock-tick, turning one $\Sigma^*$-term into another. Each transition is given
two labels, written above and below the arrow.  The upper refers to the
signals observed at the `dangling wires' on the left-hand side of the term, and
the lower to those observed on the right hand side.  Indeed, transitions
out of a term of type $m \to n$ have upper labels in $\k^m$ and lower ones in
$\k^n$.

Because---for the purposes of the operational account---we consider these terms
to be syntactic, we must also account for the twist $\twist$ and identity $\id$.
A summary of the structural rules is given below; the rules for the mirror image
generators are symmetric, in the sense that upper and lower labels are swapped.

%\begin{figure*}[ht]
\[
\copygen \dtrans{\,k\,}{k \, k} \copygen \quad 
\discardgen \dtrans{k}{} \discardgen   
\]
\[
\addgen \dtrans{k \labelSep\, l }{k+l} \addgen \quad 
\zerogen \dtrans{\phantom{b}}{0} \zerogen
\]
%\[
%\copyopgen \! \dtrans{k \labelSep k}{k} \!\copyopgen \ 
%\discardopgen \dtrans{\phantom{k}}{k} \discardopgen \ 
%\addopgen\! \dtrans{ k+l}{k \labelSep l }\! \addopgen \ 
%\zeroopgen \dtrans{0}{} \zeroopgen
%\]
\[
\scalargen  \dtrans{\,\,l\,\,}{al} \scalargen \quad 
\delaygenl \dtrans{k}{l} \delaygenk 
%\scalaropgen  \dtrans{al}{\,l\,} \scalaropgen \ 
%\circuitXopT ^{\labelSep l}\! \dtrans{l}{k} \circuitXopT ^{\labelSep k} \quad \quad
\]
%\BmultT\! \dtrans{k \labelSep k}{k} \!\BmultT \quad\quad
%\BunitT \dtrans{\phantom{k}}{k} {\BunitT} \quad\quad
%\scalaropT  \dtrans{kl}{\,l\,} \scalaropT \quad \quad
%\circuitXopT ^{\labelSep l}\! \dtrans{l}{k} \circuitXopT ^{\labelSep k} \quad \quad
%\WcomultT\! \dtrans{ k+l}{k \labelSep l }\! \WcomultT\quad\quad
%\WcounitT \dtrans{0}{} \WcounitT
%\]
\[
\id  \dtrans{k}{k} \id \quad 
\twist \dtrans{k \labelSep l}{l \labelSep k} \twist 
\]
\[
  \frac{s\dtrans{\mathbf{u}}{\mathbf{v}} s' \quad
  t\dtrans{\mathbf{v}}{\mathbf{w}} t'}{s \mathrel{;} t \dtrans
  {\mathbf{u}}{\mathbf{w}} s' \mathrel{;} t'}
 \quad 
 \frac{s\dtrans{\mathbf{u_1}}{\mathbf{v_1}} s'\quad
 t\dtrans{\mathbf{u_2}}{\mathbf{v_2}} t'}
 {s\oplus t \dtrans{\mathbf{u_1 \labelSep u_2}}{\mathbf{v_1 \labelSep v_2}} s'\oplus t'}
\]
%\caption{Summary of structural rules for the operational semantics; $k,l\in\k$.\label{fig.opsem}}
%\end{figure*}
Here $k, l,a \in \k$, $s,t$ are $\Sigma^*$-terms, and
$\mathbf{u,v,w,u_1,u_2,v_1,v_2}$
are vectors in $\k$ of the appropriate length. Note that the only generators
that change as a result of computation are the registers; it follows this is the
only source of state in any computation.

Let $\tm\colon m\to n \in \syntax$ be a $\Sigma$-term. Fixing an ordering of
the delays $\delaygen$ in $\tm$ allows us to identify the set of delays with a
finite ordinal $[d]$.  A \define{register assignment} is then simply a function
$\sigma: [d]\to \k$.  We may instantiate the $\Sigma$-term $\tm$ with the
register assignment $\sigma$ to obtain the $\Sigma^\ast$-term $\tm_\sigma
\in\syntax^\ast$ of the same type: for all $i \in [d]$ simply replace the $i$th
delay with a register in state $\sigma(i)$.

A computation on $\tm$ initialised at $\sigma$ is an infinite sequence of register
assignments and transitions: 
\[
  \tm_\sigma \dtrans{\mathbf{u_1}}{\mathbf{v_1}} \tm_{\sigma_1}
  \dtrans{\mathbf{u_2}}{\mathbf{v_2}} \tm_{\sigma_2}  
  \dtrans{\mathbf{u_3}}{\mathbf{v_3}} \dots
\]
The \define{trace} induced by this computation is the sequence 
\[(\mathbf{u_1},\mathbf{v_1}), (\mathbf{u_2},\mathbf{v_2}), \dots\] 
of elements of $\k^m \oplus \k^n$.

\smallskip
To relate the operational and denotational semantics, we introduce the
notion of biinfinite trace: a trace with an infinite past
as well as future. To define these, we use the notion of a \emph{reverse} computation: 
a computation using the operational rules above, but with the rules for delay having their
  left and right hand sides swapped:
\[
\delaygenk \dtrans{k}{l} \delaygenl.
\]

\begin{definition}
  Given $\tm\colon m \to n \in \syntax$, a \define{biinfinite trace on} $\tm$ is a sequence
$w \in (\k^m)^\z \oplus (\k^n)^\z$ such that there exists
\begin{enumerate}[(i)]
\item a register assignment $\sigma$; 
\item an infinite \emph{forward trace} $\phi_\sigma$ of a computation on $\tm$
  initialised at $\sigma$; and,
\item an infinite \emph{backward trace} $\psi_\sigma$ of a reverse computation
  on $\tm$ initialised at $\sigma$,
\end{enumerate}
obeying
\[
w(t) = \begin{cases}
\phi(t) & t\geq 0; \\
\psi(-(t+1)) & t < 0.
\end{cases}
\]
We write $\bit(\tm)$ for the set of all biinfinite traces on $\tm$.
\end{definition}

The following result gives a tight correspondence between the operational and
denotational semantics, and follows via a straightforward structural induction
on $\tm$.
\begin{lemma}
For any $\tm\maps m\to n \in \syntax$, we have 
\[
  \llbracket \tm \rrbracket = \bit(\tm) 
\]
as subsets of $(\k^m)^\z\times (\k^n)^\z$.
\end{lemma}

\section{Controllability} \label{sec.control}

Suppose we are given a current and a target trajectory for a system. Is it
always possible, in finite time, to steer the system onto the target trajectory? If so, the 
 system is deemed controllable, and the problem of controllability of systems is at
the core of control theory. The following definition is due to
Willems~\cite{Wi2}.

\begin{definition}\label{def:contr}
A system $(T,W,\bb)$ (or simply the behaviour $\bb$) is \define{controllable} if
for all $w,w' \in \bb$ and all times $t_0\in\mathbb{Z}$, there exists $w'' \in
\bb$ and $t_0^\prime\in\mathbb{Z}$ such that $t_0'>t_0$ and $w''$ obeys
\[
  w''(t) = 
  \begin{cases} 
    w(t) & t \le t_0 \\
    w'(t-t_0') & t \ge t_0'.
  \end{cases}
\]
\end{definition}
%$\bb$ is controllable if given the past of some trajectory and the future of
%another, there exists a trajectory in the behaviour that transitions, in finite
%time, from the chosen past to the chosen future. 

As mentioned previously, a novel feature of our graphical calculus is
that it allows us to consider non-controllable behaviours.

\begin{example} \label{ex.noncontrol}
Consider the system in the introduction, further elaborated in Remark~\ref{rmk:splus1}.
%represented by the signal flow
%graph~\eqref{eq:examplesfg}. The equation 
%\[
%\includegraphics[height=1.3cm]{pics/firstequality.pdf}
%\]
%follows from the equations of $\ltids$, and so as an $\ltids$ system
%\eqref{eq:examplesfg} is the system represented by the corelation
%%\[
%$1 \xrightarrow{[s+1]} 1 \xleftarrow{[s+1]} 1.$
%%\]
%
%As discussed in Remark \ref{rmk.omittedaxs}, 
As noted previously,
the trajectories of this system are
precisely those sequences $w= (w_1, w_2) \in \k^\z \oplus \k^\z$ that satisfy
the difference equation
\[
  w_1(t-1)+w_1(t)-w_2(t-1)-w_2(t)=0.
\]
To see that the system is non-controllable, note that
\[
  w_1(t-1)-w_2(t-1) = -(w_1(t)-w_2(t)),
\]
so $(w_1-w_2)(t) = (-1)^tc_w$ for some $c_w \in \k$. This $c_w$ is a time-invariant
property of any trajectory. Thus if $w$ and $w'$ are trajectories such that $c_w \ne
c_{w'}$, then it is not possible to transition from the past of $w$ to the
future of $w'$ along some trajectory in $\mathcal B$. 

Explicitly, taking $w(t) = ((-1)^t,0)$ and $w'(t) = ((-1)^t2,0)$ suffices to
show $\mathcal B$ is not controllable.
\end{example}

\subsection{A categorical characterisation}
We now show that controllable systems are precisely those representable as
\emph{spans} of matrices. This novel characterisation leads to new ways of
reasoning about controllability of composite systems. 

Among the various equivalent conditions for controllability, the existence of
\emph{image representations} is most useful for our purposes.
\begin{proposition}[Willems {\cite[p.86]{Wi}}] \label{thm.imagereps}
  An LTI behaviour $\bb$ is controllable iff there exists $M \in \mat\pk$ such
  that $\bb = \im \theta M$.
\end{proposition}

Restated in our language, Proposition \ref{thm.imagereps} states that controllable
systems are precisely those representable as \emph{spans} of matrices. 

\begin{theorem} \label{cor.spanreps}
  Let $m \xrightarrow{A} d \xleftarrow{B} n$ be a corelation in $\corel\mat\pk$.
  Then the LTI behaviour
  $\Phi(\xrightarrow{A}\xleftarrow{B})$ is controllable iff there exists $R: e
  \to m$, $S: e\to n$ such that 
  \[
    m \xleftarrow{R} e \xrightarrow{S} n = m \xrightarrow{A} d \xleftarrow{B} n
  \]
  as morphisms in $\corel\mat\pk$. 
\end{theorem}
\begin{proof}
  To begin, note that the behaviour of a span is its joint image. That is,
  $\Phi(\xleftarrow{R}\xrightarrow{S})$ is the composite of linear
  relations $\ker\vectfun[\mathrm{id}_m \; -R]$ and $\ker\vectfun[S\;
  -\mathrm{id}_n]$, which comprises all $(\mathbf{x},\mathbf{y}) \in (\k^m)^\z
  \oplus (\k^n)^\z$ s.t.\ $\exists$ $\mathbf{z} \in (\k^e)^\z$ with
  $\mathbf{x} = \vectfun R \mathbf{z}$ and $\mathbf{y} = \vectfun S
  \mathbf{z}$. Thus
  \[
    \Phi(\xleftarrow{R}\xrightarrow{S}) = \im \vectfun \left[
    \begin{matrix} R \\ S \end{matrix} \right].
  \]
  The result then follows immediately from Proposition \ref{thm.imagereps}.  
\end{proof}

In terms of the graphical theory, this means that a term in the 
form $\ha_\pk ; \ha_\pk^{op}$ (`cospan form') is controllable iff we can find a
derivation, using the rules of $\ihcor$, that puts it in the form $\ha_\pk^{op}
; \ha_\pk$ (`span form').  This provides a general, easily recognisable
representation for controllable systems. 

Span representations also lead to a test for controllability: take
the pullback of the cospan and check whether the system described by it
coincides with the original one. Indeed, note that as $\pk$ is a PID, the
category $\mat\pk$ has pullbacks. A further consequence of Theorem
\ref{cor.spanreps}, together with Proposition  \ref{prop.magic}, is the following. 

\begin{proposition} \label{prop.ctrlablepart}
  Let $m \xrightarrow{A} d \xleftarrow{B} n$ be a cospan in $\mat\pk$, and write
  the pullback of this cospan $m \xleftarrow{R} e \xrightarrow{S} n$. Then the
  behaviour of the pullback span $\Phi(\xleftarrow{R}\xrightarrow{S})$ is
  the maximal controllable sub-behaviour of
  $\Phi(\xrightarrow{A}\xleftarrow{B})$.
\end{proposition}
\begin{proof}
  Suppose we have another controllable behaviour $\mathscr{C}$ contained in
  $\ker\vectfun [A\;-B]$. Then this behaviour is the $\Phi$-image of some span
  $m \xleftarrow{R'} e' \xrightarrow{S'}n$. As $\im\vectfun\begin{bmatrix} R' \\
    S'\end{bmatrix}$ lies in $\ker\vectfun [A\;-B]$, the universal property of
  the pullback gives a map $e' \to e$ such that the relevant diagram commutes.
  This implies that the controllable behaviour $\mathscr{C} =
  \im\vectfun\begin{bmatrix} R' \\ S'\end{bmatrix}$ is contained in $\im\vectfun
  \begin{bmatrix} R \\ S\end{bmatrix}$, as required. 
\end{proof}

\begin{corollary}
  Suppose that an LTI behaviour $\bb$ has cospan representation
  \[
    m \stackrel{A}\longrightarrow d \stackrel{B}\longleftarrow n.
  \]
  Then $\bb$ is controllable iff the $\Phi$-image of the pullback of this cospan
  in $\mat\pk$ is equal to $\bb$.
\end{corollary}

Moreover, taking the pushout of this pullback span gives another cospan. The
morphism from the pushout to the original cospan, given by the universal
property of the pushout, describes the way in which the system fails to be
controllable.

To continue Remark \ref{rmk.omittedaxs}, the theory of interacting Hopf
algebras $\ih_\pk$~\cite{BSZ2,Za} may be viewed as our theory $\ihcor$ of LTI
systems together with the axioms given by the pullback in $\mat\pk$. Thus,
graphically, the pullback may be computed by using the axioms of $\ih_\pk$. For
example, the pullback span of the system of Example \ref{ex.noncontrol} is
simply the identity span, as derived in equation \eqref{eq:exampleproof} of the
previous section. In the traditional matrix calculus for control theory, one
derives this by noting the system has kernel representation
$\ker\theta\begin{bmatrix} s+1 & -(s+1) \end{bmatrix}$, and eliminating the
common factor $s+1$ between the entries.  Either way, we conclude that the
maximally controllable subsystem of $1 \xrightarrow{[s+1]} 1 \xleftarrow{[s+1]}
1$ is simply the identity system $1 \xrightarrow{[1]} 1 \xleftarrow{[1]} 1$.

% Brendan, I think we should skip this for now, we can bring it back for the journal version
%\begin{proposition}
%  Suppose we have $f\maps n \to m$ and $g\maps n \leftarrow m$ are equal as
%  corelations. Then $g = f^{-1}$.
%\end{proposition}
%\begin{proof}
%  This means the following commutes:
%  \[
%    \xymatrix{
%      & n \\
%      n \ar@{=}[ur] \ar[dr]_f & & m \ar[ul]_g \ar@{=}[dl] \\
%      & m
%    }
%  \]
%\end{proof}

\subsection{Control and interconnection}
From this vantage point we can make useful observations about controllable
systems and their composites: we simply need to ask whether we can rewrite them
as spans. 

\begin{example}
  Suppose that $\bb$ has cospan representation
  %\[
    $m \xrightarrow{A} d \xleftarrow{B} n.$
  %\]
  Then $\bb$ is easily seen to be controllable when $A$ or $B$ is invertible.
  Indeed, if $A$ is invertible, then $m  \xleftarrow{A^{-1}B} n
  \xrightarrow{\idn_n} n$ is an equivalent span; if $B$ is invertible, then
  $m\xleftarrow{\idn_m} m \xrightarrow{B^{-1}A} n$.
\end{example}

More significantly, the compositionality of our framework aids understanding of
how controllability behaves under the interconnection of systems---an active
field of investigation in current control theory. We give an example application
of our result.

First, consider the following proposition.
\begin{proposition}\label{prop:veryexciting}
  Let $\bb,\mathscr{C}$ be controllable systems, given by the respective
  $\Phi$-images of the spans
  %\[
    $m \xleftarrow{B_1} d \xrightarrow{B_2} n$ % \qquad \mbox{$
    and %} \qquad
    $n \xleftarrow{C_1} e \xrightarrow{C_2} l$.
  %\]
  Then the composite $\mathscr{C} \circ \bb: m \to l$ is controllable
  if $\Phi(\xrightarrow{B_2}\xleftarrow{C_1})$ is
  controllable.
\end{proposition}
\begin{proof}
  Replacing $\xrightarrow{B_2}\xleftarrow{C_1}$ with an equivalent span gives a span
  representation for $\mathscr{C} \circ \bb$.
\end{proof}

\begin{example}
  Consider LTI systems
  \[
  \lower25pt\hbox{$\includegraphics[height=2cm]{pics/68diag1.pdf}$}
  \quad\text{and}\quad
  \lower22pt\hbox{$\includegraphics[height=1.8cm]{pics/68diag2.pdf}$}.
  \]
  These systems are controllable because each is represented by a span in
  $\mat\pk$. Indeed, recall that each generator of $\ltids=\corel\mat\pk$ arises
  as the image of a generator in $\mat\pk$ or $\mat\pk^{\mathrm{op}}$; for
  example, the white monoid map $\addgen$ represents a morphism in $\mat\pk$,
  while the black monoid map $\copyopgen$ represents a morphism in
  $\mat\pk^{\mathrm{op}}$. The above diagrams are spans as we may partition the
  diagrams above so that each generator in $\mat\pk^{\mathrm{op}}$ lies to the
  left of each generator in $\mat\pk$.

  To determine controllability of the interconnected system
  \[
  \lower25pt\hbox{$\includegraphics[height=2.6cm]{pics/68diag3.pdf}$}
  \]
  Proposition \ref{prop:veryexciting} states that it is enough to consider the
  controllability of the subsystem
  \[
  \lower17pt\hbox{$\includegraphics[height=1.4cm]{pics/68diag4orig.pdf}$}.
  \]
  The above diagram gives a representation of the subsystem as a cospan in
  $\mat\pk$. We can prove it is controllable by rewriting it as a span using an
  equation of $\ltids$:
  \[
  \lower25pt\hbox{$\includegraphics[height=2.2cm]{pics/68diag4.pdf}$}
    =
  \lower25pt\hbox{$\includegraphics[height=2.2cm]{pics/68diag5.pdf}$}.
  \]
  Thus the composite system is controllable.
\end{example}

\begin{remark}
  Note the converse of Proposition \ref{prop:veryexciting} fails. For a simple
  example, consider the system 
  \[
  \lower10pt\hbox{$\includegraphics[height=1cm]{pics/trivial.pdf}$}
  \]
  This is equivalent to the empty system, and so trivially controllable. The
  central span, however, is not controllable (Example \ref{ex.noncontrol}).
\end{remark}

\subsection{Comparison to matrix methods}
The facility with which the graphical calculus formalises and solves such
controllability issues is especially appealing in view of potential applications
in the analysis of controllability of \emph{systems over networks} (see
\cite{OFM}). To make the reader fully appreciate such potential, we sketch how
complicated such analysis is using standard algebraic methods and
dynamical system theory even for the highly restrictive case of two systems that
compose to make a single-input, single-output system.  See also pages 513 to 516
of Fuhrmann and Helmke's recent book \cite{FH}, where a generalization of the
result of Proposition \ref{prop:veryexciting} is given in a polynomial- and
operator-theoretic setting. 

In the following we abuse notation by writing a matrix for its image under the
functor $\theta$.  The following is a useful result for analysing the
controllability of kernel representations applying only to the single-input
single-output case.

\begin{proposition}[Willems {\cite[p.75]{Wi}}] \label{prop.controlkernel}
  Let $\mathscr B \subseteq (\k^2)^\z$ be a behaviour given by the kernel of the
  matrix $\theta\begin{bmatrix} A & B\end{bmatrix}$, where $A$ and $B$ are column
  vectors with entries in $\k[s,s^{-1}]$. Then $\mathscr B$ is controllable if
  and only if the greatest common divisor $\gcd(A,B)$ of $A$ and $B$ is $1$.
\end{proposition}

Using the notation of Proposition \ref{prop:veryexciting}, the trajectories of
$\mathscr{B}$ and $\mathscr{C}$ respectively are those $(w_1,w_2) \in (\k^n)^\z
\oplus (\k^m)^\z$ and $(w_2',w_3) \in (\k^m)^\z \oplus (\k^p)^\z$ such that 
\begin{equation}\label{eq:imBnC}
\begin{bmatrix} w_1\\w_2 \end{bmatrix}
=
\begin{bmatrix} B_1\\B_2 \end{bmatrix} \ell_1 
\quad \mbox{ \rm and} \quad  
\begin{bmatrix} w_2^\prime\\w_3 \end{bmatrix}
=
\begin{bmatrix} C_1\\C_2 \end{bmatrix} \ell_2\; 
\end{equation}
for some $\ell_1 \in (\k^b)^\z$, $\ell_2 \in (\k^c)^\z$. These are the explicit
image representations of the two systems.  We assume without loss
of generality that the representations (\ref{eq:imBnC}) are \emph{observable}
(see \cite{Wi}); this is equivalent to $\gcd(B_1,B_2)=\gcd(C_1,C_2)=1$. Augmenting
(\ref{eq:imBnC}) with the interconnection constraint
$w_2=B_2\ell_1=C_1\ell_2=w_2^\prime$ we obtain the representation of the
interconnection: 
\begin{eqnarray}\label{eq:hybBnC}
\begin{bmatrix}
w_1\\w_2\\w_2^\prime\\w_3\\0
\end{bmatrix}&=&\begin{bmatrix} B_1&0\\B_2&0\\0&C_1\\ 0&C_2 \\ B_2&-C_1 \end{bmatrix} \begin{bmatrix} \ell_1\\\ell_2\end{bmatrix}\; .
\end{eqnarray}
Proposition \ref{prop:veryexciting} concerns the controllability of the set
$\mathscr{C} \circ \mathscr{B}$ of trajectories $(w_1,w_3)$ for which there
exist trajectories $w_2$, $w_2^\prime$, $\ell_1$, $\ell_2$ such that
(\ref{eq:hybBnC}) holds. 

To obtain a representation of such behaviour the variables $\ell_1$, $\ell_2$,
$w_2$ and $w_2^\prime$ must be eliminated from (\ref{eq:hybBnC}) via algebraic
manipulations (see the discussion on p. 237 of \cite{Wi2}). Denote
$G=\gcd(B_1,C_2)$, and write $C_2=G C_2^\prime$ and $B_1=G B_1^\prime$, where
$\gcd(B_1^\prime, C_2^\prime)=1$.  Without entering in the algebraic details, it
can be shown that a kernel representation of the projection of the behaviour of
(\ref{eq:hybBnC}) on the variables $w_1$ and $w_3$ is
\begin{equation}\label{eq:extafterelim}
  \begin{bmatrix} C_2^\prime B_2& -B_1^\prime C_2\end{bmatrix} 
  \begin{bmatrix} w_1\\ w_3\end{bmatrix}=0\; .
\end{equation}
We now restrict to the single-input single-output case. Recalling Proposition
\ref{prop.controlkernel}, the behaviour represented by (\ref{eq:extafterelim}) is
controllable if and only if $\gcd(C_2^\prime B_2, B_1^\prime C_1)=1$. 

Finally then, to complete our alternate proof of the single-input single-output
case of Proposition \ref{prop:veryexciting}, note that
$\cospanfunrest(\xrightarrow{B_2}\xleftarrow{C_1})$ is controllable if
$\gcd(B_2, C_1)=1$.  Given the observability assumption, this implies
$\gcd(C_2^\prime B_2, B_1^\prime C_1)=1$, and so the interconnected behaviour
$\mathscr{C} \circ \mathscr{B}$ represented by (\ref{eq:extafterelim}) is
controllable. 

In the multi-input, multi-output case stating explicit conditions on the
controllability of the interconnection given properties of the
representations of the individual systems and their interconnection is rather
complicated. This makes the simplicity of Proposition \ref{prop:veryexciting} and the
straightforward nature of its proof all the more appealing.



